import os
import time
import json
import re
import google.generativeai as genai
import streamlit as st
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import hashlib

# Configure API
genai.configure(api_key="AIzaSyCEJLy1bwbtXnjtdC6i2clawIEAgEcjwaw")

# Generation config
generation_config = {
    "temperature": 0.15,
    "top_p": 0.92,
    "top_k": 45,
    "max_output_tokens": 12000,
    "response_mime_type": "text/plain",
}

SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE
}

# Enhanced keyword mapping
KEYWORDS = {
    "‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£": ["‡∏™‡∏°‡∏±‡∏Ñ‡∏£", "‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô", "apply", "admission", "enrollment", "‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏±‡∏ö", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö", "‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÄ‡∏Ç‡πâ‡∏≤"],
    "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£": ["‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô", "documents", "‡πÉ‡∏ö‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç", "‡πÉ‡∏ö‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö", "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£"],
    "‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°": ["‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "tuition", "fee", "‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏Ñ‡πà‡∏≤‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô", "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"],
    "‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ": ["‡∏õ.‡∏ï‡∏£‡∏µ", "bachelor", "undergraduate", "‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï", "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ"],
    "‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡πÇ‡∏ó": ["‡∏õ.‡πÇ‡∏ó", "master", "graduate", "‡∏°‡∏´‡∏≤‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï", "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡πÇ‡∏ó"],
    "‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡πÄ‡∏≠‡∏Å": ["‡∏õ.‡πÄ‡∏≠‡∏Å", "PhD", "doctorate", "‡∏î‡∏∏‡∏©‡∏é‡∏µ‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï", "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡πÄ‡∏≠‡∏Å"],
    "‡∏™‡∏≤‡∏Ç‡∏≤": ["‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤", "‡πÅ‡∏ú‡∏ô‡∏Å", "program", "major", "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£", "‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏≠‡∏Å", "‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏≠‡∏Å"],
    "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°": ["‡∏ß‡∏¥‡∏®‡∏ß‡∏∞", "engineer", "engineering", "‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå"],
    "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ": ["‡πÄ‡∏ó‡∏Ñ", "technology", "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®", "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°"],
    "‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå": ["‡∏Ñ‡∏£‡∏∏", "‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "education", "‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°"],
    "‡πÑ‡∏ü‡∏ü‡πâ‡∏≤": ["electrical", "‡∏≠‡∏¥‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏£‡∏≠‡∏ô‡∏¥‡∏Å‡∏™‡πå", "electronics", "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÑ‡∏ü‡∏ü‡πâ‡∏≤"],
    "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå": ["computer", "‡∏Ñ‡∏≠‡∏°", "‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®", "IT", "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå"],
    "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏•": ["mechanical", "‡∏Å‡∏•", "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏•"],
    "‡πÇ‡∏¢‡∏ò‡∏≤": ["civil", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á", "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏¢‡∏ò‡∏≤"],
    "‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏≤‡∏£": ["industrial", "‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°", "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏≤‡∏£"],
    "‡∏™‡∏≠‡∏ö": ["‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö", "exam", "test", "‡∏ó‡∏î‡∏™‡∏≠‡∏ö", "‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö"],
    "‡πÄ‡∏Å‡∏£‡∏î": ["‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢", "GPA", "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡πÄ‡∏Å‡∏£‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥"],
    "‡∏ó‡∏∏‡∏ô": ["‡∏ó‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "scholarship", "‡∏ó‡∏∏‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ"],
    "‡∏ï‡πà‡∏≠": ["‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠", "‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πà‡∏≠", "continue", "‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠", "‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ", "‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÑ‡∏î‡πâ"],
    "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥": ["‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç", "qualification", "requirement", "‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î"],
    "‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": ["‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ß‡∏•‡∏≤", "duration", "‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤", "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"],
    "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£": ["‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô", "procedure", "‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£", "method"],
    "‡∏Ñ‡∏ì‡∏∞": ["faculty", "‡∏™‡∏≥‡∏ô‡∏±‡∏Å", "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢"],
    "‡∏™‡∏≤‡∏¢": ["‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏™‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡πÅ‡∏ú‡∏ô"]
}

# System Prompt ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
SYSTEM_PROMPT = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö KMUTNB

‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:

**1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:**

‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÜ ‚Üí ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÜ
- "‡∏°.6 ‡∏™‡∏≤‡∏¢‡∏®‡∏¥‡∏•‡∏õ‡πå‡∏ï‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á?" ‚Üí ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ì‡∏∞
- "‡∏°‡∏µ‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?" ‚Üí ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ì‡∏∞
- "‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏™‡∏≤‡∏Ç‡∏≤?" ‚Üí ‡∏ï‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô + ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤

‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‚Üí ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
- "‡∏™‡∏≤‡∏Ç‡∏≤‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?" ‚Üí ‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏ó‡∏∏‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
- "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?" ‚Üí ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠
- "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏¢‡∏±‡∏á‡πÑ‡∏á?" ‚Üí ‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

**2. ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£:**
‡∏ï‡∏≠‡∏ö‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ì‡∏∞ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "‡∏°.6 ‡∏™‡∏≤‡∏¢‡∏®‡∏¥‡∏•‡∏õ‡πå‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°"

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏ì‡∏∞/‡∏™‡∏≤‡∏Ç‡∏≤:**
‡∏ï‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô + ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ì‡∏∞/‡∏™‡∏≤‡∏Ç‡∏≤ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏¢‡πà‡∏≠‡∏¢)

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÉ‡∏ô‡∏Ñ‡∏ì‡∏∞:**
‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ó‡∏∏‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥:**
‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠ + ‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:**
‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢:**
‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô + ‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£

**3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:**
- ‡∏ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‚Üí ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ bullet points
- ‡∏ñ‡πâ‡∏≤‡∏ï‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ ‚Üí ‡πÉ‡∏ä‡πâ bullet points (‚Ä¢)
- ‡πÅ‡∏ö‡πà‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡πÄ‡∏ß‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

**4. ‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°:**
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ: ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ñ‡πà‡∏∞ ‡∏ô‡∏∞
- ‡∏´‡πâ‡∏≤‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: ‡∏´‡∏ô‡πâ‡∏≤ ‡∏Ç‡πâ‡∏≠
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
- ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

**5. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ:**

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: "‡∏à‡∏ö ‡∏°.6 ‡∏™‡∏≤‡∏¢‡∏®‡∏¥‡∏•‡∏õ‡πå‡∏ï‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á?"
‡∏ï‡∏≠‡∏ö: "‡∏™‡∏≤‡∏¢‡∏®‡∏¥‡∏•‡∏õ‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°"

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: "‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏™‡∏≤‡∏Ç‡∏≤?"
‡∏ï‡∏≠‡∏ö: "‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏°‡∏µ 22 ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô:

**‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ: 7 ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£**
- ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï (‡∏Ñ.‡∏≠.‡∏ö.) 4 ‡∏õ‡∏µ
- ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏°‡∏Ñ‡∏Ñ‡∏≤‡∏ó‡∏£‡∏≠‡∏ô‡∏¥‡∏Å‡∏™‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (TT)
...

**‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡πÇ‡∏ó: 7 ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£**
...

**‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡πÄ‡∏≠‡∏Å: 8 ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£**
..."

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: "‡∏°‡∏µ‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?"
‡∏ï‡∏≠‡∏ö: "KMUTNB ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏ì‡∏∞ ‡∏≠‡∏≤‡∏ó‡∏¥:
- ‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå
- ‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏£‡∏∏‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°
- ‡∏Ñ‡∏ì‡∏∞‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ
..."

‡∏™‡∏¥‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: **‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó**
"""

# Initialize model with chat history support
def create_chat_session():
    model = genai.GenerativeModel(
        model_name="gemini-2.5-flash",
        safety_settings=SAFETY_SETTINGS,
        generation_config=generation_config,
        system_instruction=SYSTEM_PROMPT,
    )
    return model.start_chat(history=[])

class DocumentProcessor:
    def __init__(self):
        self.cache_dir = "cache"
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def get_file_hash(self, file_path: str) -> str:
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return None
    
    def load_from_cache(self, file_hash: str) -> str:
        cache_file = os.path.join(self.cache_dir, f"{file_hash}.txt")
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                return f.read()
        return None
    
    def save_to_cache(self, file_hash: str, content: str):
        cache_file = os.path.join(self.cache_dir, f"{file_hash}.txt")
        with open(cache_file, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def read_pdf_optimized(self, file_path: str) -> str:
        if not os.path.exists(file_path):
            return "Error: File not found"
        
        file_hash = self.get_file_hash(file_path)
        if file_hash:
            cached_content = self.load_from_cache(file_hash)
            if cached_content:
                return cached_content
        
        try:
            import fitz
            doc = fitz.open(file_path)
            
            full_text = ""
            total_pages = len(doc)
            
            for page_num in range(total_pages):
                try:
                    page = doc[page_num]
                    page_text = page.get_text("text")
                    
                    if page_text.strip():
                        page_text = self.clean_text(page_text)
                        full_text += f"\n{page_text}\n"
                        
                except Exception:
                    continue
            
            doc.close()
            
            if file_hash and full_text:
                self.save_to_cache(file_hash, full_text)
            
            return full_text
            
        except ImportError:
            return "Error: ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyMuPDF (pip install PyMuPDF)"
        except Exception as e:
            return f"Error: {str(e)}"
    
    def clean_text(self, text: str) -> str:
        text = re.sub(r'\(‡∏´‡∏ô‡πâ‡∏≤\s*\d+[^)]*\)', '', text)
        text = re.sub(r'\(‡∏Ç‡πâ‡∏≠\s*[\d.]+\)', '', text)
        text = re.sub(r'‡∏´‡∏ô‡πâ‡∏≤\s*\d+[^,\n]*', '', text)
        text = re.sub(r'‡∏Ç‡πâ‡∏≠\s*[\d.]+[^,\n]*', '', text)
        text = re.sub(r'=== ‡∏´‡∏ô‡πâ‡∏≤ \d+ ===', '', text)
        text = re.sub(r'\x00', '', text)
        text = re.sub(r'\f', '\n', text)
        text = re.sub(r'\r\n', '\n', text)
        text = re.sub(r'\r', '\n', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        return text.strip()

class SmartSearcher:
    def __init__(self):
        self.max_chunk_size = 200000
    
    def analyze_query_type(self, query: str) -> dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö"""
        query_lower = query.lower()
        
        analysis = {
            "detail_level": "medium",
            "question_type": "general",
            "needs_list": False
        }
        
        broad_patterns = [
            r'‡∏ï‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ',
            r'‡∏°‡∏µ‡∏Ñ‡∏ì‡∏∞(‡∏≠‡∏∞‡πÑ‡∏£)?‡∏ö‡πâ‡∏≤‡∏á',
            r'‡∏£‡∏±‡∏ö‡∏™‡∏≤‡∏¢(‡∏≠‡∏∞‡πÑ‡∏£)?',
            r'‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÑ‡∏î‡πâ(‡∏ó‡∏µ‡πà)?‡πÑ‡∏´‡∏ô'
        ]
        
        if any(re.search(pattern, query_lower) for pattern in broad_patterns):
            analysis["detail_level"] = "low"
            analysis["question_type"] = "faculty_list"
            return analysis
        
        count_patterns = [
            r'(‡∏°‡∏µ)?‡∏Å‡∏µ‡πà(‡∏Ñ‡∏ì‡∏∞|‡∏™‡∏≤‡∏Ç‡∏≤|‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£)',
            r'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô(‡∏Ñ‡∏ì‡∏∞|‡∏™‡∏≤‡∏Ç‡∏≤|‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£)',
            r'‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏µ‡πà'
        ]
        
        if any(re.search(pattern, query_lower) for pattern in count_patterns):
            analysis["detail_level"] = "medium"
            analysis["question_type"] = "count"
            analysis["needs_list"] = True
            return analysis
        
        detailed_patterns = [
            r'(‡∏°‡∏µ|‡πÄ‡∏õ‡∏¥‡∏î)(‡∏™‡∏≤‡∏Ç‡∏≤|‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£)(‡∏≠‡∏∞‡πÑ‡∏£|‡πÑ‡∏´‡∏ô)?‡∏ö‡πâ‡∏≤‡∏á',
            r'‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥',
            r'‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£',
            r'‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢',
            r'‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°',
            r'‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô',
            r'‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£'
        ]
        
        if any(re.search(pattern, query_lower) for pattern in detailed_patterns):
            analysis["detail_level"] = "high"
            analysis["question_type"] = "detailed"
            analysis["needs_list"] = True
        
        return analysis
    
    def expand_query(self, query: str) -> list:
        expanded = [query.lower()]
        query_lower = query.lower()
        
        for keyword, synonyms in KEYWORDS.items():
            if keyword in query_lower:
                expanded.extend([s.lower() for s in synonyms])
            for syn in synonyms:
                if syn.lower() in query_lower:
                    expanded.append(keyword.lower())
                    expanded.extend([s.lower() for s in synonyms])
        
        words = query_lower.split()
        for word in words:
            if len(word) > 2:
                expanded.append(word)
        
        return list(set(expanded))
    
    def find_relevant_chunks(self, content: str, query: str, max_chunks: int = 20) -> list:
        expanded_keywords = self.expand_query(query)
        paragraphs = re.split(r'\n\s*\n', content)
        scored_chunks = []
        
        for i, paragraph in enumerate(paragraphs):
            if not paragraph.strip() or len(paragraph.strip()) < 20:
                continue
                
            paragraph_lower = paragraph.lower()
            score = 0
            matched_keywords = set()
            
            for keyword in expanded_keywords:
                keyword_lower = keyword.lower()
                exact_matches = len(re.findall(rf'\b{re.escape(keyword_lower)}\b', paragraph_lower))
                if exact_matches > 0:
                    score += exact_matches * 15
                    matched_keywords.add(keyword_lower)
                
                partial_matches = paragraph_lower.count(keyword_lower) - exact_matches
                score += partial_matches * 5
                
                if keyword_lower in query.lower():
                    score += exact_matches * 10
            
            list_indicators = ["1.", "2.", "3.", "‚Ä¢", "-", "‡∏Å.", "‡∏Ç.", "‡∏Ñ.", "‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤", "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£", "‡∏™‡∏≤‡∏Ç‡∏≤", "‡∏Ñ‡∏ì‡∏∞"]
            for indicator in list_indicators:
                if indicator in paragraph:
                    score += 5
            
            context_keywords = [
                "‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£", "‡∏™‡∏≤‡∏Ç‡∏≤", "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£", "‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç", 
                "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
                "‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "‡∏ó‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö", "‡πÄ‡∏Å‡∏£‡∏î", "‡∏Ñ‡∏ì‡∏∞"
            ]
            context_bonus = sum(5 for ctx_word in context_keywords if ctx_word in paragraph_lower)
            score += context_bonus
            
            if len(paragraph) > 200:
                score += 3
            
            score += len(matched_keywords) * 5
            
            if score > 0:
                extended_chunk = ""
                start_idx = max(0, i-1)
                end_idx = min(len(paragraphs), i+2)
                
                for j in range(start_idx, end_idx):
                    if paragraphs[j].strip():
                        extended_chunk += paragraphs[j].strip() + "\n\n"
                
                scored_chunks.append((score, i, extended_chunk.strip()))
        
        scored_chunks.sort(key=lambda x: x[0], reverse=True)
        
        selected_chunks = []
        total_length = 0
        seen_content = set()
        
        for score, para_num, chunk in scored_chunks[:max_chunks]:
            chunk_hash = hash(chunk[:100])
            if chunk_hash in seen_content:
                continue
            seen_content.add(chunk_hash)
            
            if total_length + len(chunk) > self.max_chunk_size:
                remaining_space = self.max_chunk_size - total_length
                if remaining_space > 500:
                    chunk = chunk[:remaining_space] + "..."
                    selected_chunks.append(chunk)
                break
                
            selected_chunks.append(chunk)
            total_length += len(chunk)
        
        return selected_chunks
    
    def search_and_answer(self, query: str, content: str, chat_session) -> str:
        if not content or content.startswith("Error:"):
            return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ"
        
        query_analysis = self.analyze_query_type(query)
        relevant_chunks = self.find_relevant_chunks(content, query)
        
        if not relevant_chunks:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"
        
        combined_content = "\n\n=== ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ===\n\n".join(relevant_chunks)
        
        if query_analysis["detail_level"] == "low":
            instruction = """
‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô:
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡∏ï‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á" ‚Üí ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£)
- ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡∏°‡∏µ‡∏Ñ‡∏ì‡∏∞‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á" ‚Üí ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ì‡∏∞
- ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏´‡∏£‡∏∑‡∏≠ bullet points ‡∏á‡πà‡∏≤‡∏¢‡πÜ
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏¢‡πà‡∏≠‡∏¢
"""
        elif query_analysis["detail_level"] == "medium":
            instruction = """
‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á:
- ‡∏ö‡∏≠‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏ß‡∏° (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
- ‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ì‡∏∞/‡∏™‡∏≤‡∏Ç‡∏≤/‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
- ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
- ‡πÉ‡∏ä‡πâ bullet points
"""
        else:
            instruction = """
‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:
- ‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡∏ó‡∏∏‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
- ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
- ‡πÉ‡∏ä‡πâ bullet points ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
"""
        
        prompt = f"""
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å KMUTNB:
{combined_content}

‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
{instruction}

‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°:
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ: ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ñ‡πà‡∏∞ ‡∏ô‡∏∞
- ‡∏´‡πâ‡∏≤‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: ‡∏´‡∏ô‡πâ‡∏≤ ‡∏Ç‡πâ‡∏≠ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ñ‡∏≤‡∏° ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ:
"""
        
        try:
            response = chat_session.send_message(prompt)
            return self.clean_response(response.text)
        except Exception as e:
            return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"
    
    def clean_response(self, response: str) -> str:
        if not response:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
        
        response = re.sub(r'\(‡∏´‡∏ô‡πâ‡∏≤\s*\d+[^)]*\)', '', response)
        response = re.sub(r'\(‡∏Ç‡πâ‡∏≠\s*[\d.]+\)', '', response)
        response = re.sub(r'‡∏´‡∏ô‡πâ‡∏≤\s*\d+[^,\s]*', '', response)
        response = re.sub(r'‡∏Ç‡πâ‡∏≠\s*[\d.]+[^,\s]*', '', response)
        
        unwanted_patterns = [
            r'‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£[^.]*\.?',
            r'‡∏ï‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£[^.]*\.?',
            r'‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏[^.]*\.?',
            r'‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô[^.]*\.?',
            r'\b‡∏Ñ‡∏£‡∏±‡∏ö\b',
            r'\b‡∏Ñ‡πà‡∏∞\b',
            r'\b‡∏ô‡∏∞\b'
        ]
        
        for pattern in unwanted_patterns:
            response = re.sub(pattern, '', response, flags=re.IGNORECASE)
        
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line and not any(line.startswith(word) for word in ['‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á', '‡∏î‡∏π‡∏à‡∏≤‡∏Å', '‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà', '‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•']):
                line = re.sub(r'\s*\([^)]*‡∏´‡∏ô‡πâ‡∏≤[^)]*\)$', '', line)
                line = re.sub(r'\s*\([^)]*‡∏Ç‡πâ‡∏≠[^)]*\)$', '', line)
                if line:
                    cleaned_lines.append(line)
        
        response = '\n'.join(cleaned_lines)
        response = re.sub(r'\n\s*\n\s*\n+', '\n\n', response)
        response = re.sub(r' +', ' ', response)
        
        return response.strip()

class RateLimiter:
    def __init__(self):
        if 'api_calls' not in st.session_state:
            st.session_state.api_calls = []
    
    def can_make_request(self) -> bool:
        current_time = time.time()
        st.session_state.api_calls = [
            call_time for call_time in st.session_state.api_calls 
            if current_time - call_time < 60
        ]
        return len(st.session_state.api_calls) < 20
    
    def add_request(self):
        st.session_state.api_calls.append(time.time())
    
    def get_wait_time(self) -> int:
        if not st.session_state.api_calls:
            return 0
        current_time = time.time()
        oldest_call = min(st.session_state.api_calls)
        return max(0, int(60 - (current_time - oldest_call)))

# Initialize components
doc_processor = DocumentProcessor()
searcher = SmartSearcher()
rate_limiter = RateLimiter()

def new_chat():
    st.session_state["messages"] = [
        {"role": "assistant", "content": "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠"}
    ]
    st.session_state["chat_session"] = create_chat_session()
    st.rerun()

# Page config
st.set_page_config(
    page_title="KMUTNB Chatbot",
    page_icon="üéì",
    layout="centered"
)

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
    
    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ text_input ‡πÄ‡∏•‡∏¢ ‡πÅ‡∏Ñ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ï‡∏£‡∏á‡πÜ
    file_path = "Dataset.pdf"

    if st.button("üîÑ ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà", use_container_width=True):
        if 'document_content' in st.session_state:
            del st.session_state.document_content
        st.rerun()
    if st.button("üí¨ New Chat", use_container_width=True):
        new_chat()
    
    if 'document_content' in st.session_state:
        content = st.session_state.document_content
        if content and not content.startswith("Error:"):
            paragraphs = len([p for p in re.split(r'\n\s*\n', content) if p.strip()])


# Main app
st.title("üéì KMUTNB Chatbot")
st.caption("‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö KMUTNB")

# Load document
if 'document_content' not in st.session_state:
    if file_path.strip():
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF..."):
            content = doc_processor.read_pdf_optimized(file_path)
            st.session_state.document_content = content
            
            if content.startswith("Error:"):
                st.error(f"‚ùå {content}")
            else:
                paragraphs = len([p for p in re.split(r'\n\s*\n', content) if p.strip()])

# Initialize chat session
if "chat_session" not in st.session_state:
    st.session_state["chat_session"] = create_chat_session()

# Initialize messages
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠"}
    ]

# Display messages
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# Chat input
if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°..."):
    if 'document_content' not in st.session_state:
        st.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô")
        st.stop()
    
    if not rate_limiter.can_make_request():
        wait_time = rate_limiter.get_wait_time()
        st.error(f"‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠ {wait_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        st.stop()
    
    # Add user message
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    
    # Generate response
    with st.chat_message("assistant"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."):
            response = searcher.search_and_answer(
                prompt, 
                st.session_state.document_content,
                st.session_state["chat_session"]
            )
            rate_limiter.add_request()
            
            st.write(response)
            st.session_state["messages"].append({"role": "assistant", "content": response})